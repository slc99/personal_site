[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Liam Connor",
    "section": "",
    "text": "I aspire to work on challenging data science problems with a positive social impact. Please see my projects for a sample of my work.\nI graduated from Seattle University with a triple major in Math, Economics, and Philosophy. I assisted in economics research and served as rowing team president. After graduating, I worked as an analyst at DaVita. There, I modeled complex multi-million dollar negotiations, automated and expanded team capabilities, and earned ‘Rookie of the Year’. I am now earning an M.S. in Economics and Computer Science at Duke, a two-year, in-person program."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Liam Connor",
    "section": "Education",
    "text": "Education\n\nDuke University | M.S. in Economics and Computer Science | May 2026 (Expected)\n\nCoursework in deep learning, natural language processing, and machine learning.\nRecipient of the Master’s Scholar Award, a merit-based tuition scholarship.\n\n\n\nSeattle University | B.S. in Mathematics, B.A. in Economics, Third Major in Philosophy | December 2022\n\nSumma Cum Laude, 3.9 GPA.\nUniversity Honors Program curriculum focused on policy analysis.\nEarned two degrees and three majors across three distinct schools."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Liam Connor",
    "section": "Experience",
    "text": "Experience\n\nAnalyst, Strategy and Analytics | DaVita | April 2023 - July 2024\n\nProvided analytical and strategic support for negotiating contracts valued in the hundreds of millions. Worked with finance and actuarial teams to model negotiations.\nDeveloped an algorithm to assess facility capacity and patient travel time, which provided critical strategic insights into the entire $7B+ portfolio. Utilized Excel, Python, SQL, and Tableau.\nAutomated a key financial modeling process, saving over 300 labor hours annually.\nRecognized as ‘Rookie of the Year’. Selected as one of three high-performing internal analysts for a yearlong leadership development program (completed June 2024).\n\n\n\nResearch Assistant | Seattle University | March 2021 - December 2022\n\nApplied causal discovery algorithms to social science datasets. Led initial research by identifying algorithms, processing data, and developing reusable Python code.\nSupported research on policing and asset forfeiture by conducting exploratory data analysis on a novel US crime database. Used R for data analysis.\n\n\n\nMen’s Crew President | Seattle University | September 2020 - June 2022\n\nManaged a 20-person team. Led recruitment, built a collaborative team culture, and steered club direction.\nLaunched new fundraisers and secured the club’s largest-ever budget.\n\n\n\nEconomics Tutor & Teaching Assistant | Seattle Univeristy | September 2020 - June 2022\n\nTutored 20 students in microeconomics, macroeconomics, and econometrics through the Seattle University Economics Department.\nServed as a teaching assistant for intermediate microeconomics."
  },
  {
    "objectID": "index.html#awards",
    "href": "index.html#awards",
    "title": "Liam Connor",
    "section": "Awards",
    "text": "Awards\n\nRookie of the Year (January 2024): Awarded to the top-performing first-year analyst on the DaVita Payor Partnerships team.\nReichmann Award (June 2022): Honors a graduating philosophy department student for academic excellence, based on faculty evaluation and GPA.\nAmerican Collegiate Rowing All-American (May 2019): Criteria included athletic performance and coach recommendation.\nMember of Alpha Sigma Nu and Omicron Delta Epsilon honor societies."
  },
  {
    "objectID": "index.html#technical-skills",
    "href": "index.html#technical-skills",
    "title": "Liam Connor",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nPython (Pandas, NumPy, PyTorch, Scikit-learn, Matplotlib, Plotly, Seaborn)\nSQL (Oracle DB)\nEconometrics, statistics, and machine learning\nEconomic modeling\nTableau\nExcel\nR"
  },
  {
    "objectID": "index.html#personal-interests",
    "href": "index.html#personal-interests",
    "title": "Liam Connor",
    "section": "Personal Interests",
    "text": "Personal Interests\nAfter a decade of rowing, I have recently started cycling and running. I enjoy all sorts of outdoor activities, such as hiking, biking, camping, and especially skiing. I like to play board games, card games, and video games. I am an avid audiobook listener, particularly of history books. I love traveling and seeing new places."
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Courses",
    "section": "",
    "text": "Duke Univeristy\n\nCOMPSCI 590: Theory of Deep Learning\nCOMPSCI 571: Probabilistic Machine Learning\nCOMPSCI 572: Natural Language Processing\nCOMPSCI 531: Introduction to Algorithms\nECON 701: Microeconomic Analysis I\nECON 605: Advanced Microeconomic Analysis\n\n\n\nSeattle University\n\nECON 4910: Causal Inference\nECON 4110: Applied Econometrics\nMATH 3411: Probability\nMATH 2340: Differential Equations\nMATH 4440: Applied Fourier Analysis\nMATH 2320: Linear Algebra\nMATH 4431: Intro to Real Analysis I\nMATH 4432: Intro to Real Analysis II\nECON 4590: Market Power and Response\nMATH 3910: Optimal Estimation and Control"
  },
  {
    "objectID": "projects/quote-machine/index.html#introduction",
    "href": "projects/quote-machine/index.html#introduction",
    "title": "Quote Machine",
    "section": "Introduction",
    "text": "Introduction\n\nI love the website Quote Investigator. Author Garson O’Toole has performed careful research to track down the true origin of over 2,000 user submitted quotes. While browsing the site, I noticed that many misattributed quotes had a similar style, while actual historical quotes were much more varied in word choice and phrasing.1 I wanted to train a model along the lines of this intuition. Ideally, the model could serve as a first pass on a quote’s legitimacy. To accomplish this, I built a natural language processing model that predicts a quote author’s gender, birth year, and occupation. First, I web-scraped 230k quotes from Wikiquote and data on 12k authors from Wikimedia. Next, I built a PyTorch model using a BERT encoding backbone and multiple prediction heads. Then, I trained the model on remote GPUs and monitored performance with Weights and Biases. Finally, I deployed the model with an interactive UI via Gradio and Hugging Face Spaces.\nThe first step in learning from history is to recognize our shared humanity with the past. A good quote can help bridge the gap to past generations and lead us to see the applicability of their experience. A misattributed quote can often do the opposite. It may further the mythicization of historical figures and, consequently, their irrelevance."
  },
  {
    "objectID": "projects/quote-machine/index.html#data",
    "href": "projects/quote-machine/index.html#data",
    "title": "Quote Machine",
    "section": "Data",
    "text": "Data\n\nData Scraping\nQuotes were scraped from Wikiquote. Wikiquote was selected over the popular Goodreads dataset for two reasons: first, Wikiquote provides a Wikimedia ID, allowing accurate author-attribute matching; second, Goodreads has many misattributed quotes, which runs counter to the original motivation.\nI used BeautifulSoup to scrape Wikiquote. I first collected individual profile links by iterating through an index of people by name. Then, I scraped text from the edit page instead of the primary page because the edit page was more easily parsable. The code for the scraping is below.\n\n\nWikiquote scraping code\n# ==============================\n# First collect all page links\n# ==============================\n\nBASE_URL = \"https://en.wikiquote.org\"\nSTART_URL = f\"{BASE_URL}/wiki/List_of_people_by_name\"\nEDIT_URL_TEMPLATE = \"https://en.wikiquote.org/w/index.php?title={}&action=edit\"\nPEOPLE_LINK_PATH = Path(\"data/scraped_data/people_edit_links.txt\")\n\n\ndef get_people_links(url):\n    \"\"\"Extracts and returns (name, title) pairs from a Wikiquote page.\n    Title is the formatted page name.\n    \"\"\"\n    headers = {}  # Write header here\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n\n    soup = BeautifulSoup(response.text, \"html.parser\")\n    people_links = set()\n\n    content_div = soup.find(\"div\", {\"id\": \"mw-content-text\"})\n\n    if not content_div:\n        print(f\"No content found in {url}\")\n        return people_links\n\n    for a_tag in content_div.find_all(\"a\", href=True):\n        href = a_tag[\"href\"]\n        if href.startswith(\"/wiki/\") and not any(\n            prefix in href for prefix in [\":\", \"#\"]\n        ):\n            title = href.split(\"/wiki/\")[1]\n            name = a_tag.get_text(strip=True)\n            people_links.add((name, title))  # store as tuple (name, title)\n\n    return people_links\n\n\ndef collect_wikiquotes_edit_links():\n    print(f\"Collecting links from {START_URL}...\")\n    people_pairs = get_people_links(START_URL)\n\n    additional_pairs = set()\n    final_pairs = set()\n\n    for name, title in list(people_pairs):\n        if \"List_of_people_by_name\" in title:\n            # Handle index pages: Add their links to the queue for further processing\n            link = urljoin(BASE_URL, \"/wiki/\" + title)\n            print(f\"Checking additional page: {link}\")\n            time.sleep(1)\n            more_pairs = get_people_links(link)\n            additional_pairs.update(more_pairs)\n        else:\n            # Handle individual pages: Add them to the final output\n            final_pairs.add((name, title))\n\n    # Combine all links from additional pages, excluding index pages\n    for name, title in additional_pairs:\n        if \"List_of_people_by_name\" not in title:\n            final_pairs.add((name, title))\n\n    edit_urls = []\n    with open(PEOPLE_LINK_PATH, \"w\", encoding=\"utf-8\") as f:\n        for name, title in sorted(final_pairs, key=lambda x: x[0]):  # sort by name\n            edit_url = EDIT_URL_TEMPLATE.format(title)\n            edit_urls.append(edit_url)\n            f.write(f\"{name}\\t{edit_url}\\n\")  # write name and edit URL separated by tab\n\n    print(f\"Collected {len(edit_urls)} unique edit URLs.\")\n\n\n# Read the file and parse it into a list of tuples\n# PEOPLE_LINK_PATH = \"data\\\\scraped_data\\\\people_edit_links.txt\"\nif os.path.exists(PEOPLE_LINK_PATH):\n    pass\nelse:\n    collect_wikiquotes_edit_links()\n\n\nwith open(PEOPLE_LINK_PATH, \"r\", encoding=\"utf-8\") as file:\n    people_edit_links = [tuple(line.strip().split(\"\\t\")) for line in file]\nnames = [x[0] for x in people_edit_links]\nurls = [x[1] for x in people_edit_links]\n\n\n# ==============================\n# Now, parse each edit page\n# ==============================\n\n\ndef query_wikiquote(url: str) -&gt; BeautifulSoup | None:\n    \"\"\"Takes an edit-page URL and returns a BeautifulSoup object.\"\"\"\n    # parse and extract quotes\n    headers = {\"header here\"}\n    try:\n        response = requests.get(url, headers=headers, timeout=10)\n    except Exception:\n        print(f\"Failed to fetch the page (likely timeout): {url}\")\n        return None\n\n    if response.status_code != 200:\n        print(\"Failed to fetch the page.\")\n        return None\n\n    soup = BeautifulSoup(response.text, \"html.parser\")\n\n    return soup\n\n\ndef extract_textarea(soup: BeautifulSoup) -&gt; str:\n    \"\"\"Extracts the textarea content from the BeautifulSoup object.\"\"\"\n    textarea = soup.find(\"textarea\", {\"name\": \"wpTextbox1\"})\n\n    if not textarea:\n        print(\"Couldn't find the textarea.\")\n        return \"\"\n\n    return textarea.text\n\n\ndef extract_wikimedia_id(soup: BeautifulSoup) -&gt; str:\n    \"\"\"Extracts the Wikimedia ID from the soup object.\"\"\"\n    # Find the &lt;a&gt; tag with the desired href\n    link = soup.find(\"a\", class_=\"extiw wb-entity-link external\")\n    # Extract the Q-number from the href attribute\n    q_number = \"\"\n    if link and \"href\" in link.attrs:\n        href = link[\"href\"]\n        q_number = href.split(\"/\")[-1]  # Extract the last part of the URL\n    return q_number\n\n\noutput_csv = Path(\"data/scraped_data/wikiquotes_scraped.csv\")\nseen_names = set()\n\n# This loop was necessary to resume scraping after interruptions\nwhile len(seen_names) + 50 &lt; len(names):\n    # Check if the file exists to determine whether to write the header\n    file_exists = os.path.isfile(output_csv)\n\n    if file_exists:\n        scraped_data = pd.read_csv(output_csv, encoding=\"utf-8\")\n        seen_names.update(scraped_data[\"name\"].tolist())\n\n    with open(output_csv, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n        fieldnames = [\"name\", \"wikimedia_id\", \"raw_text\"]\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n        if not file_exists:\n            writer.writeheader()\n\n        for name, url in zip(names, urls):\n            if name in seen_names:\n                print(f\"Skipping {name} as it has already been processed.\")\n                continue\n            print(f\"Processing {name} from {url}...\")\n            if not url.startswith(\"https://en.wikiquote.org/w/index.php?title=\"):\n                print(f\"Skipping {name} as the URL is not an edit page.\")\n                continue\n            soup = query_wikiquote(url)\n            if soup is None:\n                print(f\"Skipping {name} due to query failure.\")\n                continue\n            wikimedia_id = extract_wikimedia_id(soup)\n            raw_text = extract_textarea(soup)\n            writer.writerow(\n                {\"name\": name, \"wikimedia_id\": wikimedia_id, \"raw_text\": raw_text},\n            )\n            time.sleep(0.5)\n\nresults_df = pd.read_csv(output_csv, encoding=\"utf-8\")\nwikimedia_ids = set(results_df[\"wikimedia_id\"].tolist())\n\nwikimedia_id_path = Path(\"data/scraped_data/wikimedia_ids.txt\")\nwith open(wikimedia_id_path, \"w\", encoding=\"utf-8\") as f:\n    for wikimedia_id in wikimedia_ids:\n        f.write(f\"{wikimedia_id}\\n\")\n\n\nI accessed Wikidata through their API. I retrieved data on each author’s birth year, death year, occupation, nationality, gender, and average monthly page views.\n\n\nData Cleaning\nOwing to the crowdsourced nature of Wikiquote, the initial data pull was very messy. I performed the following processing steps:\n\nParse individual quotes and their associated headers from the full edit text. Recording the header was necessary because many pages include a “Misattributed” or “Quotes About [Person]” section that should be filtered.\nMark bolded quote segments. Many Wikiquote entries include a memorable bolded quote alongside significant context. I recorded and marked the bolded fragments for later processing.\nClean common markup artifacts.\nFilter non-English quotes.\nPerform systematic checks and spot checks to remove other abnormalities from the scraped data, such as external links, specific poorly formatted authors, and quotes with messy headers.\n\nCleaning the author data was much simpler.\n\n\nQuote data cleaning\ndef text_to_list(text: str) -&gt; list[str]:\n    \"\"\"\n    Parse markdown text into a list of section headers and quotes.\n    Finds all strings that begin with \"=\" (section headers) or \"*\" (quotes).\n    \"\"\"\n    return re.findall(r\"^[=*].*$\", text, re.MULTILINE)\n\n\ndef quotes_from_list(section_headers_and_quotes: list[str]) -&gt; list[dict]:\n    \"\"\"\n    Extract quotes with their associated metadata from parsed text.\n    Processes a list of headers and quotes, organizing them into dictionaries\n    with header, subheader, and notes information.\n    \"\"\"\n    header = \"\"\n    subheader = \"\"\n    recorded_quotes: list[dict] = []\n    for dirty in section_headers_and_quotes:\n        # Headers\n        if re.match(r\"^==[^=].*[^=]==$\", dirty):\n            header = dirty.strip(\"= \")\n        # Subheaders\n        elif re.match(r\"^===[^=].*[^=]===$\", dirty):\n            subheader = dirty.strip(\"= \")\n        # Quotes\n        elif re.match(r\"^\\*\\*[^*]\", dirty):\n            if recorded_quotes:\n                recorded_quotes[-1][\"notes\"].append(dirty.strip(\"* \"))\n        elif re.match(r\"^\\*[^*]\", dirty):\n            quote_dict = {\n                \"raw_quote\": dirty.strip(\"* \"),\n                \"header\": header,\n                \"subheader\": subheader,\n                \"notes\": [],\n            }\n            recorded_quotes.append(quote_dict)\n    return recorded_quotes\n\n\ndef join_fragments(fragments: list[str], original_text: str) -&gt; str:\n    \"\"\"\n    Join bolded quote fragments with appropriate separators.\n    Combines multiple bolded subsections into a single quote, using different\n    separators based on whether fragments are part of the same sentence.\n    \"\"\"\n    result = []\n    last_end = 0\n\n    for i, fragment in enumerate(fragments):\n        if i &gt; 0:\n            # Find the text between the last bold fragment and the current one\n            start = original_text.find(f\"'''{fragment}'''\", last_end)\n            in_between_text = original_text[last_end:start]\n            last_end = start + len(f\"'''{fragment}'''\")\n\n            # Check if the in-between text contains a sentence-ending punctuation\n            if re.search(r\"[.!?]\", in_between_text) and not re.search(\n                r\"[.!?]$\", result[-1]\n            ):\n                result.append(\" .... \")  # Different sentence\n            else:\n                result.append(\" ... \")  # Same sentence\n\n        result.append(fragment)\n    return \"\".join(result)\n\n\ndef parse_to_bold(q_dict: dict) -&gt; list[dict]:\n    \"\"\"\n    Extract and mark bolded quote sections.\n    Creates separate entries for complete quotes and their bolded fragments,\n    with flags indicating whether text is bolded.\n    \"\"\"\n\n    def fill_dict(\n        quote_to_add: str, orig_dict: dict, bold_fragment: bool, bold_combined: bool\n    ) -&gt; dict:\n        \"\"\"Helper function to create quote dictionary with bold metadata.\"\"\"\n        bold = bold_fragment or bold_combined\n        quote_dict = {\n            \"raw_quote\": quote_to_add,\n            \"raw_complete_quote\": orig_dict[\"raw_quote\"],\n            \"bold\": bold,\n            \"bold_fragment\": bold_fragment,\n            \"bold_combined\": bold_combined,\n            \"header\": q_dict[\"header\"],\n            \"subheader\": q_dict[\"subheader\"],\n            \"notes\": q_dict[\"notes\"],\n        }\n        return quote_dict\n\n    recorded_with_bold: list[dict] = []\n    quote = q_dict[\"raw_quote\"]\n    quote_dict = fill_dict(quote, q_dict, False, False)\n    recorded_with_bold.append(quote_dict)\n    bold_fragments = re.findall(r\"'''(.*?)'''\", quote)\n\n    # If there are multiple bold sections, add them separately and combined\n    if len(bold_fragments) &gt; 1:\n        # Add the combined quote\n        combined_fragments = join_fragments(bold_fragments, quote)\n        quote_dict = fill_dict(combined_fragments, q_dict, False, True)\n        recorded_with_bold.append(quote_dict)\n        for bold_fragment in bold_fragments:\n            # Add each fragment independently\n            quote_dict = fill_dict(bold_fragment, q_dict, True, False)\n            recorded_with_bold.append(quote_dict)\n\n    # Add single bolded section\n    elif len(bold_fragments) == 1:\n        quote_dict = fill_dict(bold_fragments[0], q_dict, True, True)\n        recorded_with_bold.append(quote_dict)\n\n    return recorded_with_bold\n\n\ndef cleaner(dirty: str) -&gt; str:\n    \"\"\"\n    Remove common markup formatting from text.\n    Strips wiki markup, HTML tags, special characters, and extra whitespace\n    while preserving basic punctuation and readability.\n    \"\"\"\n    dirty = (\n        dirty.replace(\"'''\", \"\")\n        .replace(\"''\", \"\")\n        .strip(\"*\")\n        .lstrip()\n        .replace(\"[[w:\", \"\")\n    )\n    dirty = re.sub(r\"&lt;br&gt;\", \" \", dirty)\n    dirty = re.sub(r\"{{.*?}}\", \"\", dirty)\n    dirty = re.sub(r\"&lt;/?[^&gt;]+&gt;\", \"\", dirty)\n    dirty = re.sub(r\"\\|.*?\\]\\]\", \"\", dirty)\n    dirty = re.sub(\n        r\"[^0-9A-Za-z\\s\\.,\\?!:;\\'\\\"\\(\\)\\-\\u2013\\u2014\\/\\u2018\\u2019\\u201c\\u201d\\u2026&]\",\n        \"\",\n        dirty,\n    )\n    clean = re.sub(r\"\\s+\", \" \", dirty).strip()\n    return clean\n\n\ndef clean_bold_dict(input_dict: dict) -&gt; dict:\n    \"\"\"\n    Apply cleaner function to all text fields in quote dictionary.\n    \"\"\"\n    q_dict = input_dict.copy()\n\n    q_dict[\"header\"] = cleaner(q_dict[\"header\"])\n    q_dict[\"subheader\"] = cleaner(q_dict[\"subheader\"])\n    cleaned_notes = []\n    for note in q_dict[\"notes\"]:\n        cleaned_notes.append(cleaner(note))\n    q_dict[\"notes\"] = cleaned_notes\n\n    if not q_dict[\"raw_quote\"]:\n        return q_dict\n    q_dict[\"clean_quote\"] = cleaner(q_dict[\"raw_quote\"])\n    q_dict[\"clean_complete_quote\"] = cleaner(q_dict[\"raw_complete_quote\"])\n\n    return q_dict\n\n\ndef page_pipeline(text: str) -&gt; list[dict]:\n    \"\"\"\n    Complete processing pipeline for a Wikiquote page.\n    Parses raw edit page text through all cleaning and processing steps\n    to extract structured quote data.\n    \"\"\"\n    section_headers_and_quotes: list[str] = text_to_list(text)\n    quote_dicts: list[dict] = quotes_from_list(section_headers_and_quotes)\n    cleaned_bold_dicts: list[dict] = []\n    for q_dict in quote_dicts:\n        bold_dicts: list[dict] = parse_to_bold(q_dict)\n        for bold in bold_dicts:\n            cleaned_bold: dict = clean_bold_dict(bold)\n            cleaned_bold_dicts.append(cleaned_bold)\n    return cleaned_bold_dicts\n\n\n# Read raw scraped data\ncsv_path = os.path.join(\"data\", \"scraped_data\", \"wikiquotes_scraped.csv\")\nraw_quotes = pd.read_csv(csv_path)\n\n# Apply the pipeline and explode the results into a new DataFrame\nprocessed = raw_quotes.apply(\n    lambda row: [\n        {**elem, \"wikimedia_id\": row[\"wikimedia_id\"], \"name\": row[\"name\"]}\n        for elem in page_pipeline(row[\"raw_text\"])\n    ],\n    axis=1,\n)\n\n# Flatten the list of lists and create a DataFrame\nflat_list = [item for sublist in processed for item in sublist]\nquotes_df = pd.DataFrame(flat_list)\n\n\ndef filter_about(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter out quotes about people rather than by them.\n    Removes sections containing quotes discussing a person rather than\n    quotes spoken or written by that person.\n    \"\"\"\n    about_filter = r\"^\\s*(Quotations regarding Spinoza|Song lyrics about Lars Ulrich|Film directors about|Some more interesting facts about Mukesh Ambani|Comments About|Quotes from stars|quotes about|quotations about|about|quotes of others|quotations of others|quotes from others|quotes by others|quotation about|said about)\"\n    return df[~df[\"header\"].str.contains(about_filter, case=False, na=False)]\n\n\ndef filter_external_links(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Remove external links and 'See also' sections.\n    \"\"\"\n    df = df[df[\"header\"] != \"External links\"]\n    df = df[df[\"header\"] != \"See also\"]\n    return df\n\n\ndef filter_works(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Remove pages that contain works instead of quotes.\n    Filters out entries where the content is primarily a list of works\n    rather than actual quotations, with specific exceptions.\n    \"\"\"\n    df = df[\n        ~df[\"header\"].str.contains(\"work\", case=False, na=False)\n        | ~df[\"name\"].isin(\n            [\n                \"Malek, Marcin\",\n                \"Kok, Ingrid de\",\n                \"Maharaj, Nisargadatta\",\n                \"Rice, Tim\",\n                \"Waberi, Abdourahman\",\n            ]\n        )\n    ]\n    return df\n\n\ndef filter_fragments(\n    df: pd.DataFrame, full_cutoff: int = 3, bold_cutoof: int = 5\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Remove quote fragments that are too short.\n    Filters quotes with fewer than 3 words, or fewer than 5 words for\n    bold fragments which tend to be shorter memorable phrases.\n    \"\"\"\n    df = df[\n        ~(\n            (df[\"clean_quote\"].str.split().str.len() &lt; full_cutoff)\n            | (\n                (df[\"bold_fragment\"] == True)\n                & (df[\"clean_quote\"].str.split().str.len() &lt; bold_cutoof)\n            )\n        )\n    ]\n    return df\n\n\ndef filter_starting_numbers(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Remove leading numbers and periods from quotes.\n    \"\"\"\n    df[\"clean_quote\"] = df[\"clean_quote\"].str.replace(r\"^\\d+\\.\\s*\", \"\", regex=True)\n    return df\n\n\ndef filter_non_string(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Remove entries where clean_quote is not a string.\n    \"\"\"\n    return df[df[\"clean_quote\"].apply(lambda x: isinstance(x, str))]\n\n\ndef add_misattributed_column(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Add boolean column indicating misattributed or disputed quotes.\n    Marks quotes from 'Misattributed' or 'Disputed' sections.\n    \"\"\"\n    df[\"misattributed\"] = df[\"header\"].isin([\"Misattributed\", \"Disputed\"]) | (\n        df[\"subheader\"] == \"Misattributed\"\n    )\n    return df\n\n\ndef add_is_en_col(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Add boolean column indicating whether quote is in English.\n    Uses langdetect library to identify English quotes.\n    \"\"\"\n\n    def detect_language(text):\n        \"\"\"Detect if text is English, handling exceptions.\"\"\"\n        if isinstance(text, float):\n            return False\n        try:\n            return detect(text) == \"en\"\n        except LangDetectException:\n            return False\n\n    amended = df.copy(deep=True)\n    amended[\"is_en\"] = amended[\"clean_quote\"].progress_apply(detect_language)\n\n    return amended\n\n\ndef filter_pipeline(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply all filtering and processing steps to quote DataFrame.\n    Runs the complete sequence of filters to produce a clean dataset\n    ready for model training.\n    \"\"\"\n    df = filter_about(df)\n    df = filter_external_links(df)\n    df = filter_works(df)\n    df = filter_fragments(df)\n    df = filter_starting_numbers(df)\n    df = filter_non_string(df)\n    df = add_misattributed_column(df)\n    df = add_is_en_col(df)\n    return df\n\n\nquotes_df = filter_pipeline(quotes_df)\n\n\n\n\nAuthor data cleaning\ndef split_lowercase_list(df: pd.DataFrame, column: str) -&gt; pd.DataFrame:\n    df[column] = df[column].apply(\n        lambda x: [item.strip().lower() for item in x.split(\",\")] if pd.notna(x) else x\n    )\n    return df\n\n\ndef replace_empty_list_with_null(df: pd.DataFrame, column: str) -&gt; pd.DataFrame:\n    df = df.copy()\n    df[column] = df[column].apply(\n        lambda x: np.nan if isinstance(x, list) and not x else x\n    )\n    return df\n\n\ndef limit_by_frequency(\n    df: pd.DataFrame, column: str, min_freq: int, suffix: str = \"_limited\"\n) -&gt; pd.DataFrame:\n    \"\"\"Creates a new column that only includes values with at least `min_freq` occurrences.\"\"\"\n    df = df.copy()\n    val_counts = pd.Series([x for y in df[column].dropna() for x in y]).value_counts()\n    vals_above_min = val_counts[val_counts &gt;= min_freq].index.tolist()\n    df[column + suffix] = df[column].apply(\n        lambda lst: (\n            [val for val in lst if val in vals_above_min]\n            if isinstance(lst, list)\n            else lst\n        )\n    )\n    return df\n\n\ndef limit_by_top_k(\n    df: pd.DataFrame, column: str, k: int, suffix: str = \"_limited\"\n) -&gt; pd.DataFrame:\n    \"\"\"Creates a new column that only includes values in the list of the top-k most frequent values.\"\"\"\n    df = df.copy()\n    val_counts = pd.Series([x for y in df[column].dropna() for x in y]).value_counts()\n    top_k_vals = val_counts.head(k).index.tolist()\n    df[column + suffix] = df[column].apply(\n        lambda lst: (\n            [val for val in lst if val in top_k_vals] if isinstance(lst, list) else lst\n        )\n    )\n    return df\n\n\ndef replace_val(\n    df: pd.DataFrame, column: str, val, replace_with: float = np.nan\n) -&gt; pd.DataFrame:\n    df = df.copy()\n    df[column].replace(val, replace_with)\n    return df\n\n\ndef create_limited_dummies(\n    df: pd.DataFrame, column: str, vals: list, suffix: str = \"_dummy\"\n) -&gt; pd.DataFrame:\n    df = df.copy()\n    for val in vals:\n        df[column + suffix + \"_\" + val] = (df[column] == val).astype(int)\n    df.loc[df[column].isna()] = np.nan\n    return df\n\n\ndef fill_na(df: pd.DataFrame, column: str, func=pd.Series.median) -&gt; pd.DataFrame:\n    df = df.copy()\n    fill_value = func(df[column])\n    df[column] = df[column].fillna(fill_value)\n    return df\n\n\ndef add_log(df: pd.DataFrame, columns: list[str]) -&gt; pd.DataFrame:\n    df = df.copy()\n    for col in columns:\n        df[f\"log_{col}\"] = np.log(df[col])\n\n    return df\n\n\ndef mapping(\n    df: pd.DataFrame,\n    column_name: str,\n    map_dict: dict[str, str],\n    rsuffix: str = \"_mapped\",\n) -&gt; pd.DataFrame:\n\n    return df.join(df[column_name].map(map_dict), rsuffix=rsuffix)\n\n\nauthors = pd.read_csv(os.path.join(\"data\", \"scraped_data\", \"wikimedia_details.csv\"))\nauthors = (\n    authors.pipe(split_lowercase_list, column=\"occupation\")\n    .pipe(replace_empty_list_with_null, column=\"occupation\")\n    .pipe(limit_by_top_k, column=\"occupation\", k=50)\n    .pipe(replace_val, column=\"avg_monthly_pageviews\", val=0.0, replace_with=1e-8)\n    .pipe(create_limited_dummies, column=\"gender\", vals=[\"male\", \"female\"])\n    .pipe(add_log, columns=[\"avg_monthly_pageviews\"])\n)\n\n\n\n\nData Processing for Training\nNow that I had clean(er) data, it was time to prepare the data for model training. I performed the following steps:\n\nCreate a multi-label target for occupation: Due to Wikidata being crowdsourced, the author data contained hundreds of obscure occupations. To create reasonable targets, I filtered to the 200 most common occupations (dropping 290 of 11,761 authors) and then had &lt;an LLM&gt; categorize occupations. I then manually tweaked categories. I selected this approach over an occupation co-occurrence clustering approach, as the manual method produced more semantically meaningful categories.\nCreate category weights to address occupation skew: The most popular category, Literature & Writing, has 100x as many samples as the least popular category, Athletes & Sports. To address this, I added a scaling factor for occupation categories, defined as \\(\\text{w}_i = \\frac{\\max w}{\\text{count}^\\alpha}\\), where \\(\\alpha \\in [0, 1]\\) is a scaling hyperparameter. These weights were used to scale loss during backpropagation. A full breakdown of categories, associated quote counts, and weights is provided below.\n\n\n\nOccupation category mapping\noccupation_mapping = {\n    \"writer\": \"Literature & Writing\",\n    \"poet\": \"Literature & Writing\",\n    \"novelist\": \"Literature & Writing\",\n    \"essayist\": \"Literature & Writing\",\n    \"children's writer\": \"Literature & Writing\",\n    \"short story writer\": \"Literature & Writing\",\n    \"prose writer\": \"Literature & Writing\",\n    \"diarist\": \"Literature & Writing\",\n    \"science fiction writer\": \"Literature & Writing\",\n    \"librettist\": \"Literature & Writing\",\n    \"author\": \"Literature & Writing\",\n    \"literary scholar\": \"Literature & Writing\",\n    \"humorist\": \"Literature & Writing\",\n    \"journalist\": \"Journalism & Media\",\n    \"editor\": \"Journalism & Media\",\n    \"columnist\": \"Journalism & Media\",\n    \"blogger\": \"Journalism & Media\",\n    \"opinion journalist\": \"Journalism & Media\",\n    \"television presenter\": \"Journalism & Media\",\n    \"radio personality\": \"Journalism & Media\",\n    \"podcaster\": \"Journalism & Media\",\n    \"pundit\": \"Journalism & Media\",\n    \"publisher\": \"Journalism & Media\",\n    \"editing staff\": \"Journalism & Media\",\n    \"painter\": \"Visual Arts & Design\",\n    \"sculptor\": \"Visual Arts & Design\",\n    \"photographer\": \"Visual Arts & Design\",\n    \"illustrator\": \"Visual Arts & Design\",\n    \"draftsperson\": \"Visual Arts & Design\",\n    \"visual artist\": \"Visual Arts & Design\",\n    \"printmaker\": \"Visual Arts & Design\",\n    \"graphic artist\": \"Visual Arts & Design\",\n    \"designer\": \"Visual Arts & Design\",\n    \"architect\": \"Visual Arts & Design\",\n    \"architectural draftsperson\": \"Visual Arts & Design\",\n    \"fashion designer\": \"Visual Arts & Design\",\n    \"lithographer\": \"Visual Arts & Design\",\n    \"cartoonist\": \"Visual Arts & Design\",\n    \"comics artist\": \"Visual Arts & Design\",\n    \"artist\": \"Visual Arts & Design\",\n    \"singer\": \"Music\",\n    \"composer\": \"Music\",\n    \"songwriter\": \"Music\",\n    \"singer-songwriter\": \"Music\",\n    \"musician\": \"Music\",\n    \"guitarist\": \"Music\",\n    \"pianist\": \"Music\",\n    \"record producer\": \"Music\",\n    \"recording artist\": \"Music\",\n    \"lyricist\": \"Music\",\n    \"conductor\": \"Music\",\n    \"rapper\": \"Music\",\n    \"jazz musician\": \"Music\",\n    \"organist\": \"Music\",\n    \"disc jockey\": \"Music\",\n    \"violinist\": \"Music\",\n    \"film score composer\": \"Music\",\n    \"actor\": \"Actor\",\n    \"film actor\": \"Actor\",\n    \"television actor\": \"Actor\",\n    \"stage actor\": \"Actor\",\n    \"voice actor\": \"Actor\",\n    \"screenwriter\": \"Film & Theater Writer\",\n    \"playwright\": \"Film & Theater Writer\",\n    \"film director\": \"Film, TV, & Theater Production\",\n    \"film producer\": \"Film, TV, & Theater Production\",\n    \"television producer\": \"Film, TV, & Theater Production\",\n    \"television director\": \"Film, TV, & Theater Production\",\n    \"theatrical director\": \"Film, TV, & Theater Production\",\n    \"film editor\": \"Film, TV, & Theater Production\",\n    \"executive producer\": \"Film, TV, & Theater Production\",\n    \"choreographer\": \"Film, TV, & Theater Production\",\n    \"dramaturge\": \"Film, TV, & Theater Production\",\n    \"director\": \"Film, TV, & Theater Production\",\n    \"producer\": \"Film, TV, & Theater Production\",\n    \"physicist\": \"Science & Math\",\n    \"theoretical physicist\": \"Science & Math\",\n    \"nuclear physicist\": \"Science & Math\",\n    \"astrophysicist\": \"Science & Math\",\n    \"chemist\": \"Science & Math\",\n    \"biologist\": \"Science & Math\",\n    \"biochemist\": \"Science & Math\",\n    \"zoologist\": \"Science & Math\",\n    \"botanist\": \"Science & Math\",\n    \"geneticist\": \"Science & Math\",\n    \"geologist\": \"Science & Math\",\n    \"astronomer\": \"Science & Math\",\n    \"naturalist\": \"Science & Math\",\n    \"mathematician\": \"Science & Math\",\n    \"statistician\": \"Science & Math\",\n    \"philosopher\": \"Humanities\",\n    \"historian\": \"Humanities\",\n    \"theologian\": \"Humanities\",\n    \"linguist\": \"Humanities\",\n    \"classical scholar\": \"Humanities\",\n    \"art historian\": \"Humanities\",\n    \"archaeologist\": \"Humanities\",\n    \"economist\": \"Social Sciences\",\n    \"sociologist\": \"Social Sciences\",\n    \"psychologist\": \"Social Sciences\",\n    \"political scientist\": \"Social Sciences\",\n    \"anthropologist\": \"Social Sciences\",\n    \"geographer\": \"Social Sciences\",\n    \"sociologist\": \"Social Sciences\",\n    \"music theorist\": \"Music\",\n    \"university teacher\": \"Academia & Education\",\n    \"teacher\": \"Academia & Education\",\n    \"professor\": \"Academia & Education\",\n    \"pedagogue\": \"Academia & Education\",\n    \"lecturer\": \"Academia & Education\",\n    \"academic\": \"Academia & Education\",\n    \"researcher\": \"Academia & Education\",\n    \"music educator\": \"Academia & Education\",\n    \"librarian\": \"Academia & Education\",\n    \"educator\": \"Academia & Education\",\n    \"lawyer\": \"Law & Jurisprudence\",\n    \"judge\": \"Law & Jurisprudence\",\n    \"jurist\": \"Law & Jurisprudence\",\n    \"barrister\": \"Law & Jurisprudence\",\n    \"poet lawyer\": \"Law & Jurisprudence\",\n    \"politician\": \"Politics & Government\",\n    \"diplomat\": \"Politics & Government\",\n    \"statesperson\": \"Politics & Government\",\n    \"monarch\": \"Politics & Government\",\n    \"civil servant\": \"Politics & Government\",\n    \"theologian\": \"Religion\",\n    \"catholic priest\": \"Religion\",\n    \"anglican priest\": \"Religion\",\n    \"rabbi\": \"Religion\",\n    \"pastor\": \"Religion\",\n    \"missionary\": \"Religion\",\n    \"preacher\": \"Religion\",\n    \"religious leader\": \"Religion\",\n    \"catholic bishop\": \"Religion\",\n    \"minister\": \"Religion\",\n    \"christian minister\": \"Religion\",\n    \"cleric\": \"Religion\",\n    \"priest\": \"Religion\",\n    \"businessperson\": \"Business & Finance\",\n    \"entrepreneur\": \"Business & Finance\",\n    \"banker\": \"Business & Finance\",\n    \"merchant\": \"Business & Finance\",\n    \"manufacturer\": \"Business & Finance\",\n    \"engineer\": \"Technology\",\n    \"computer scientist\": \"Technology\",\n    \"programmer\": \"Technology\",\n    \"inventor\": \"Technology\",\n    \"activist\": \"Social Advocacy\",\n    \"human rights defender\": \"Social Advocacy\",\n    \"trade unionist\": \"Social Advocacy\",\n    \"peace activist\": \"Social Advocacy\",\n    \"political activist\": \"Social Advocacy\",\n    \"women's rights activist\": \"Social Advocacy\",\n    \"environmentalist\": \"Social Advocacy\",\n    \"revolutionary\": \"Politics & Government\",\n    \"abolitionist\": \"Social Advocacy\",\n    \"military personnel\": \"Military\",\n    \"military officer\": \"Military\",\n    \"naval officer\": \"Military\",\n    \"army officer\": \"Military\",\n    \"military leader\": \"Military\",\n    \"soldier\": \"Military\",\n    \"association football player\": \"Athletes & Sports\",\n    \"american football player\": \"Athletes & Sports\",\n    \"basketball player\": \"Athletes & Sports\",\n    \"cricketer\": \"Athletes & Sports\",\n    \"boxer\": \"Athletes & Sports\",\n    \"dancer\": \"Athletes & Sports\",\n    \"baseball player\": \"Athletes & Sports\",\n    \"association football coach\": \"Athletes & Sports\",\n    \"literary critic\": \"Literature & Writing\",\n    \"non-fiction writer\": \"Literature & Writing\",\n    \"scientist\": \"Science & Math\",\n}\n\n\n\n\nQuote count and weight by occupation category\ncounts = defaultdict(int)\ntotal_counted = 0\n\nfor authors_occupations in df[\"occupation\"].dropna():\n    quote_counted = 0\n    seen_categories = set()\n    for occupation in authors_occupations:\n        category = occupation_mapping.get(occupation)\n        if category and category not in seen_categories:\n            counts[category] += 1\n            quote_counted = 1\n            seen_categories.add(category)\n    total_counted += quote_counted\n\nfor (occupation, count), weight in zip(\n    sorted(counts.items(), key=lambda x: x[1], reverse=True),\n    sorted(occupation_weights, reverse=False),\n):\n    print(\n        f\"{occupation:&lt;30}: {count / sum(counts.values()):&gt;6.1%}. {weight:&gt;5.1f} weight, {count:&gt;7,.0f} quotes\"\n    )\n\"\"\"\nLiterature & Writing          :  23.7%.   1.0 weight, 174,633 quotes\nPolitics & Government         :   8.8%.   1.6 weight,  65,293 quotes\nJournalism & Media            :   8.8%.   1.6 weight,  64,734 quotes\nHumanities                    :   8.4%.   1.7 weight,  62,323 quotes\nAcademia & Education          :   7.9%.   1.7 weight,  58,063 quotes\nFilm & Theater Writer         :   7.1%.   1.8 weight,  52,273 quotes\nActor                         :   3.9%.   2.5 weight,  28,533 quotes\nMusic                         :   3.8%.   2.5 weight,  28,148 quotes\nSocial Sciences               :   3.7%.   2.5 weight,  26,948 quotes\nFilm, TV, & Theater Production:   3.4%.   2.6 weight,  25,393 quotes\nScience & Math                :   3.4%.   2.6 weight,  25,018 quotes\nLaw & Jurisprudence           :   3.2%.   2.7 weight,  23,372 quotes\nVisual Arts & Design          :   3.0%.   2.8 weight,  22,269 quotes\nSocial Advocacy               :   2.6%.   3.0 weight,  19,126 quotes\nReligion                      :   2.3%.   3.2 weight,  17,316 quotes\nMilitary                      :   2.0%.   3.4 weight,  14,946 quotes\nBusiness & Finance            :   1.8%.   3.7 weight,  12,984 quotes\nTechnology                    :   1.5%.   3.9 weight,  11,341 quotes\nAthletes & Sports             :   0.7%.   5.8 weight,   5,169 quotes\n\"\"\"\n\n\n\nCreate birth year targets: Ideally, the model would predict when a quote was said. I use the author’s birth year as a proxy, given the available data. I wanted the birth year target to meet the following constraints:\n\nPenalize the same absolute error more when the year is closer to the present day, so that predicting 200 CE for 400 CE is minor, but predicting 1800 CE for 2000 CE is major.\nPredict a value with a continuous domain, to enable encoding the continuous development of language. This ruled out binarized approaches.\n\n\nTo achieve these goals, I defined \\(\\text{Year label}= \\log (b-x) - \\log(b-c)\\), where \\(c = 2025\\) (the current year) is used to set the intercept, and \\(b\\) is a hyperparameter that determines the steepness of the slope, which captures the relative weight of date differences in the present day vs. the far past. I \\(z\\)-normalized the converted target.2\n\nWeight quotes by author’s popularity and number of quotes: There were two problems with the current data. First, the number of quotes per author varies by orders of magnitude. Second, I wanted “higher quality” authors (as measured by Wikipedia page views) to be upsampled. To achieve this, I set \\(w_{\\text{quote}} = \\frac{\\log_2(\\text{author page views})}{\\text{total author quotes}}\\). Authors with missing page view counts were assumed to be missing uniformly at random.\n\n\n\nBirth year target\nauthors[\"birth_year_target\"] = authors[\"birth_year\"].apply(\n    lambda x: np.log((2050 - x)) - np.log((2050 - 2025)) if pd.notnull(x) else np.nan\n)\nscaler_fn = StandardScaler()\nauthors[\"birth_year_target\"] = scaler_fn.fit_transform(authors[[\"birth_year_target\"]])\n\ndf = pd.merge(\n    quotes, authors, left_on=\"wikimedia_id\", right_on=\"wikidata_id\", how=\"left\"\n)\n\nbase = 2\ndf[f\"log_{base}_views\"] = df[\"avg_monthly_pageviews\"].apply(\n    lambda x: np.log(x) / np.log(base) if pd.notnull(x) else 1\n)\ndf[\"num_quotes\"] = df.groupby(\"wikimedia_id\")[\"wikimedia_id\"].transform(\"count\")\ndf[\"weight\"] = df[\"log_2_views\"] / df[\"num_quotes\"]"
  },
  {
    "objectID": "projects/quote-machine/index.html#model",
    "href": "projects/quote-machine/index.html#model",
    "title": "Quote Machine",
    "section": "Model",
    "text": "Model\n\nModel Design\nThe model uses a pretrained sentence-embedding model as a backbone. I selected the Sentence Transformers model all-MiniLM-L6-v2 because it is lightweight and popular. Encoded quotes are then passed through a set of shared add-on layers, composed of linear, ReLU, and dropout layers. The depth and width of these layers were parameterized. The last shared layer feeds into three prediction heads for gender, year, and occupation.\n\n\nPyTorch model class\nclass AuthorPredictor(nn.Module):\n    def __init__(\n        self,\n        encoder,\n        occupation_classes,\n        device,\n        add_on_layer_dims: list[int] | None = [128],\n        dropout_p: float = 0.1,\n        return_embeddings: bool = False,\n        extra_occupations_layer: bool = True,\n        use_hetrosk_year_loss: bool = False,\n    ):\n        super(AuthorPredictor, self).__init__()\n        self.encoder = encoder\n        self.device = device\n        self.dropout = nn.Dropout(dropout_p)\n        self.return_embeddings = return_embeddings\n        self.add_on_layer_dims = add_on_layer_dims\n        self.use_hetrosk_year_loss = use_hetrosk_year_loss\n        self.occupation_classes = occupation_classes\n\n        encoded_dim = encoder.config.hidden_size\n\n        if add_on_layer_dims:\n            layers = []\n            in_dim = encoded_dim\n            for out_dim in add_on_layer_dims:\n                layers.extend(\n                    [\n                        nn.Linear(in_dim, out_dim),\n                        nn.ReLU(),\n                        nn.Dropout(dropout_p),\n                    ]\n                )\n                in_dim = out_dim\n\n            self.add_on_layers = nn.Sequential(*layers)\n\n        latent_dim = add_on_layer_dims[-1] if add_on_layer_dims else encoded_dim\n\n        # Prediction heads\n        if extra_occupations_layer:\n            self.occupation_head = nn.Sequential(\n                nn.Linear(latent_dim, latent_dim),\n                nn.ReLU(),\n                nn.Dropout(dropout_p),\n                nn.Linear(latent_dim, occupation_classes),\n            )\n        else:\n            self.occupation_head = nn.Linear(latent_dim, occupation_classes)\n\n        year_out = 2 if use_hetrosk_year_loss else 1\n        self.birth_year_head = nn.Linear(latent_dim, year_out)\n        self.gender_head = nn.Linear(latent_dim, 1)\n\n    def forward(self, tokens, attention_mask=None):\n        # Convert tokens to embeddings\n        embeddings = self.encoder(\n            tokens, attention_mask=attention_mask\n        ).last_hidden_state[:, 0, :]\n\n        task_embeddings = self.dropout(embeddings)\n\n        if self.add_on_layer_dims:\n            task_embeddings = self.add_on_layers(task_embeddings)\n\n        # Forward through each head\n        birth_pred = self.birth_year_head(task_embeddings)\n        occupation_pred = self.occupation_head(task_embeddings)\n\n        # Detach embeddings so that noisy gender does not affect the backbone\n        gender_pred = self.gender_head(task_embeddings.detach())\n\n        # Allow embeddings to be returned\n        if self.return_embeddings:\n            return birth_pred, gender_pred, occupation_pred, embeddings\n\n        return birth_pred, gender_pred, occupation_pred\n\n\n\n\nTraining Parameters\nLoss functions \n\nGender loss: The gender prediction head used binary cross-entropy (BCE). After empirical testing, the noisiness of the gender head was preventing the model from learning. To address this, I detached the embeddings before feeding them into the gender head, isolating gender backpropagation to the prediction head only.\nOccupation loss: I tested two loss functions for the occupation head, BCE and focal loss. Loss was calculated per occupation category, scaled by the previously calculated weights, and then averaged. This prevented the model from ignoring less common occupations.\nBirth year loss: Birth year loss was calculated using heteroskedastic negative log-likelihood (HNLL). HNLL requires the model to predict a normal distribution for each sample, allowing the model to strategically express uncertainty in its predictions. HNLL was chosen over MSE to reduce regression to the mean. HNLL also has the added benefit of including a measure of model uncertainty.\n\nTraining Regime\n\nPhases: The training regime was split into two phases. In the first phase, only the added layers and prediction heads are modified. In the second phase, the encoder model is also modified.\nOptimizer: AdamW was selected as the optimizer. The learning rate for the encoder, add-on layers, and each prediction head are set separately.\nScheduling: The learning rate was scheduled with a linear warmup (10% of total batches) and linear decay (90% of batches). Scheduling reset between phases. The second phase had a proportionally lower learning rate than the first phase. The first phase lasted four epochs (empirically set based on when performance plateaued). The second phase lasted up to 12 epochs, depending on early stopping.\nEvaluation: Model performance was evaluated against a withheld validation set. The validation set contained 15% of the overall data. Model performance was calculated as the weighted average of task performance.\n\nGender: AUROC\nYear: MAE\nOccupation: Hamming distance, with multi-label “macro” reduction. This places more emphasis on uncommon labels\n\n\n\n\nTraining schedule and run functions\nclass TrainSchedule:\n    def __init__(\n        self,\n        model,\n        epochs_last_only,\n        epochs_joint,\n        train_dataloader,\n        val_dataloader,\n        birth_year_loss_fn,\n        gender_loss_fn,\n        occupation_loss_fn,\n        lr,\n        device,\n        reporting_and_early_stopping_weights: dict[str, float],\n        backprop_weights: dict[str, float],\n        joint_lr_multiplier,\n        early_stopping_epochs: int | None,\n        occupation_class_weights: torch.Tensor,\n    ):\n        self.model = model\n        self.epochs_last_only = epochs_last_only\n        self.epochs_joint = epochs_joint\n        self.train_dataloader = train_dataloader\n        self.val_dataloader = val_dataloader\n        self.birth_year_loss_fn = birth_year_loss_fn\n        self.gender_loss_fn = gender_loss_fn\n        self.occupation_loss_fn = occupation_loss_fn\n        self.device = device\n        self.early_stopping_epochs = early_stopping_epochs\n        self.lr = lr\n        self.metric_weights = reporting_and_early_stopping_weights\n        self.backprop_weights = backprop_weights\n        self.joint_lr_multiplier = joint_lr_multiplier\n        self.occupation_class_weights = occupation_class_weights\n\n    def epoch(self, train: bool):\n        if train:\n            dataloader = self.train_dataloader\n            optimizer = self.optimizer\n            label = \"train\"\n            self.model.train()\n        else:\n            dataloader = self.val_dataloader\n            optimizer = None\n            label = \"val\"\n            self.model.eval()\n\n        dl_gender = 0.0\n        dl_occupation = 0.0\n        dl_year = 0.0\n        dl_gender_weighted = 0.0\n        dl_occupation_weighted = 0.0\n        dl_year_weighted = 0.0\n\n        # Metrics\n        gender_metric = BinaryAUROC().to(device=self.device)\n        year_metric = MeanAbsoluteError().to(device=self.device)\n        year_std_metric = StdMetric().to(device=self.device)\n        occupation_metric = MultilabelHammingDistance(\n            num_labels=self.model.occupation_classes, average=\"macro\"\n        ).to(device=self.device)\n\n        pbar = tqdm(dataloader, desc=\"Avg loss\")\n\n        for batch_idx, data in enumerate(pbar):\n            if optimizer:\n                optimizer.zero_grad()\n\n            # Extract targets and tokens\n            tokens = data[\"tokens\"].to(self.device)\n            attention_mask = data[\"attention_mask\"].to(self.device)\n            birth_target = data[\"birth_year_target\"].to(self.device).unsqueeze(1)\n            gender_target = data[\"gender_dummy_male\"].to(self.device).unsqueeze(1)\n            occupation_target = data[\"occupation_target\"].to(self.device)\n\n            # Calculate model predictions\n            birth_pred, gender_pred, occupation_pred = self.model(\n                tokens, attention_mask=attention_mask\n            )\n\n            # Calculate loss per prediction head\n            year_loss = self.birth_year_loss_fn(birth_pred, birth_target)\n            gender_loss = self.gender_loss_fn(gender_pred, gender_target)\n            occupation_loss = self.occupation_loss_fn(\n                occupation_pred, occupation_target\n            )\n            occupation_loss = occupation_loss * self.occupation_class_weights\n            occupation_loss = occupation_loss.mean()\n\n            # Weight the losses\n            year_loss_weighted = year_loss * self.backprop_weights.get(\"year\", 1.0)\n            gender_loss_weighted = gender_loss * self.backprop_weights.get(\n                \"gender\", 1.0\n            )\n            occupation_loss_weighted = occupation_loss * self.backprop_weights.get(\n                \"occupation\", 1.0\n            )\n\n            # Create loss used for backprop\n            loss = year_loss_weighted + gender_loss_weighted + occupation_loss_weighted\n\n            # Backprop\n            if optimizer:\n                loss.backward()\n                optimizer.step()\n                if self.scheduler is not None:\n                    self.scheduler.step()\n\n            # Calculate reporting metrics\n            if torch.no_grad():\n                if self.model.use_hetrosk_year_loss:\n                    birth_pred = birth_pred[:, 0].unsqueeze(dim=1)\n\n                year_metric.update(target=birth_target, preds=birth_pred)\n                year_std_metric.update(birth_pred)\n\n                gender_metric.update(target=gender_target, preds=gender_pred)\n                occupation_metric.update(\n                    target=occupation_target, preds=occupation_pred\n                )\n\n            dl_gender += gender_loss.item()\n            dl_occupation += occupation_loss.item()\n            dl_year += year_loss.item()\n\n            dl_gender_weighted += gender_loss_weighted.item()\n            dl_occupation_weighted += occupation_loss_weighted.item()\n            dl_year_weighted += year_loss_weighted.item()\n\n            # For tqdm reporting\n            pbar_dict = {\n                \"gender\": f\"{dl_gender / (batch_idx + 1):.4f}\",\n                \"occupation\": f\"{dl_occupation / (batch_idx + 1):.4f}\",\n                \"year\": f\"{dl_year / (batch_idx + 1):.4f}\",\n            }\n            pbar.set_postfix(pbar_dict)\n\n        # End of epoch metrics\n        n_batches = len(dataloader)\n        gender_metric_score = gender_metric.compute().item()\n        year_metric_score = year_metric.compute().item()\n        year_std_score = year_std_metric.compute().item()\n        occupation_metric_score = occupation_metric.compute().item()\n\n        target_metric = (\n            year_metric_score * self.metric_weights.get(\"year\", 1.0)\n            + gender_metric_score * self.metric_weights.get(\"gender\", 1.0)\n            + occupation_metric_score * self.metric_weights.get(\"occupation\", 1.0)\n        )\n\n        wandb_metrics = {\n            f\"{label}_target_metric\": target_metric,\n            f\"{label}_gender_loss\": dl_gender / n_batches,\n            f\"{label}_occupation_loss\": dl_occupation / n_batches,\n            f\"{label}_year_loss\": dl_year / n_batches,\n            f\"{label}_gender_loss_weighted\": dl_gender_weighted / n_batches,\n            f\"{label}_occupation_loss_weighted\": dl_occupation_weighted / n_batches,\n            f\"{label}_year_loss_weighted\": dl_year_weighted / n_batches,\n            f\"{label}_gender_auc\": gender_metric_score,\n            f\"{label}_year_MAE\": year_metric_score,\n            f\"{label}_occupation_hamming\": occupation_metric_score,\n            f\"{label}_year_std\": year_std_score,\n        }\n        return wandb_metrics\n\n    def phase(self):\n        if not self.joint_flag:\n            total_epochs = self.epochs_last_only\n        else:\n            total_epochs = self.epochs_joint\n\n        best_target_metric = float(\"inf\")\n        epochs_since_best = 0\n        for epoch_n in range(total_epochs):\n\n            start = time.time()\n\n            print(f\"Epoch {epoch_n}\")\n            train_metrics = self.epoch(train=True)\n\n            with torch.no_grad():\n                val_metrics = self.epoch(train=False)\n\n            target_metric = val_metrics[\"val_target_metric\"]\n            print(f\"Target metric: {target_metric:.2f}\", end=\" | \")\n\n            end = time.time()\n            cur_is_best = False\n            avg_learning_rate = sum(\n                [group[\"lr\"] for group in self.optimizer.param_groups]\n            ) / len([group[\"lr\"] for group in self.optimizer.param_groups])\n\n            # Save best model\n            if best_target_metric &gt; target_metric:\n                best_target_metric = target_metric\n                cur_is_best = True\n                save_path = wandb.run.dir + \"/best_model.pt\"\n                torch.save(self.model.state_dict(), save_path)\n                print(f\"New best model saved to {save_path}\")\n\n            wandb.log(\n                {\n                    **train_metrics,\n                    **val_metrics,\n                    \"best_target_metric\": best_target_metric,\n                    \"time\": end - start,\n                    \"epoch\": epoch_n,\n                    \"phase\": \"joint\" if self.joint_flag else \"last-only\",\n                    \"avg_lr\": avg_learning_rate,\n                }\n            )\n\n            # Early stopping\n            if (self.early_stopping_epochs is not None) and self.joint_flag:\n                if cur_is_best:\n                    epochs_since_best = 0\n                else:\n                    epochs_since_best += 1\n                    print(\n                        f\"No improvement in target metric for {epochs_since_best} epochs.\"\n                    )\n                    if epochs_since_best &gt;= self.early_stopping_epochs:\n                        print(\"Early stopping triggered.\")\n                        break\n\n    def train(self):\n        if self.epochs_last_only &gt; 0:\n            print(\"Starting phase 1: Training added layers only.\")\n            # Freeze parameters\n            for param in self.model.encoder.parameters():\n                param.requires_grad = False\n\n            self.optimizer, self.scheduler = build_optimizer_scheduler(\n                model=self.model,\n                lr=self.lr,\n                total_steps=len(self.train_dataloader) * self.epochs_last_only,\n                warmup_portion=0.1,\n            )\n\n            self.joint_flag = False\n            self.phase()\n\n        if self.epochs_joint &gt; 0:\n            print(\"Starting phase 2: Joint training of all layers.\")\n            for param in self.model.encoder.parameters():\n                param.requires_grad = True\n\n            joint_lr = {\n                key: val * self.joint_lr_multiplier for key, val in self.lr.items()\n            }\n            self.optimizer, self.scheduler = build_optimizer_scheduler(\n                model=self.model,\n                lr=joint_lr,\n                total_steps=len(self.train_dataloader) * self.epochs_joint,\n                warmup_portion=0.05,\n            )\n\n            self.joint_flag = True\n            self.phase()\n\n        print(\"Training complete\")\n\ndef run(\n    batch_size: int = 32,\n    use_big_bone: bool = False,  # Use larger backbone model\n    lr: dict[str, float] = {\n        \"encoder\": 1e-4,\n        \"add_on_layers\": 1e-3,\n        \"occupation_head\": 1e-5,\n        \"birth_year_head\": 1e-3,\n        \"gender_head\": 1e-3,\n    },\n    add_on_layer_dims: list[int] | None = [\n        256,\n        128,\n    ],  # Latent dimension of each add-on layer\n    early_stopping_epochs: int = 5,  # Does not include initial epochs_last_only\n    epochs_last_only: int = 4,  # Epochs to train added layers and heads\n    epochs_joint: int = 12,  # Maxium epochs to train full model\n    data_limit: int | None = None,  # Limit dataset size for quick debugging\n    dropout_p: float = 0.25,\n    target_metric_weights: dict[str, float] = {\n        \"year\": 1.0,\n        \"gender\": 0.0,\n        \"occupation\": 2.0,  # To account for scale issues\n    },  # Weights for target early stopping (not backprop)\n    occupation_backprop_weight: float = 5.0,\n    extra_occupations_layer: bool = False,\n    occupation_loss_type: str = \"focal\",  # 'BCE', 'focal'\n    joint_lr_multiplier: float = 0.1,\n    use_occupation_weights: bool = False,\n    occupation_classes: int = 19,\n    use_hetrosk_year_loss: bool = True,\n):\n    # Initialize variables from settings\n    backprop_weights = {\n        \"year\": 1.0,\n        \"gender\": 1.0,  # Only impacts the prediction head\n        \"occupation\": occupation_backprop_weight,\n    }\n    if not add_on_layer_dims:\n        epochs_last_only = 0\n    occupation_weights = (\n        \"data/processed_data/occupation_weights.pkl\" if use_occupation_weights else None\n    )\n    DATA_PATH = \"data/processed_data/quote_dataset.parquet\"\n    if use_big_bone:\n        model_str = \"all-mpnet-base-v2\"\n    else:\n        model_str = \"all-MiniLM-L6-v2\"\n\n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Load tokenizer and encoder\n    tokenizer = AutoTokenizer.from_pretrained(f\"sentence-transformers/{model_str}\")\n    encoder = AutoModel.from_pretrained(f\"sentence-transformers/{model_str}\").to(device)\n\n    # Get dataloaders\n    train_dataloader, val_dataloader = get_dataloaders(\n        dataframe_path=DATA_PATH,\n        tokenizer=tokenizer,\n        batch_size=batch_size,\n        data_limit=data_limit,\n    )\n\n    # Initialize model\n    model = AuthorPredictor(\n        encoder=encoder,\n        occupation_classes=occupation_classes,\n        device=device,\n        add_on_layer_dims=add_on_layer_dims,\n        dropout_p=dropout_p,\n        extra_occupations_layer=extra_occupations_layer,\n        use_hetrosk_year_loss=use_hetrosk_year_loss,\n    )\n    model = model.to(device)\n\n    # Initialize loss functions\n    if use_hetrosk_year_loss:\n        birth_year_loss_fn = HeteroskedasticNLL()\n    else:\n        birth_year_loss_fn = torch.nn.MSELoss()\n\n    gender_loss_fn = torch.nn.BCEWithLogitsLoss()\n\n    if occupation_loss_type == \"BCE\":\n        occupation_loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n    elif occupation_loss_type == \"focal\":\n        occupation_loss_fn = FocalLoss(\n            gamma=2, alpha=0.25, task_type=\"multi-label\", reduction=\"none\"\n        )\n        backprop_weights[\"occupation\"] *= 10  # Adjust for focal loss\n    else:\n        raise AssertionError\n\n    if occupation_weights:\n        with open(occupation_weights, \"rb\") as f:\n            loaded_weights = pickle.load(f)\n        occupation_class_weights = torch.tensor(loaded_weights).to(device)\n    else:\n        occupation_class_weights = torch.ones(occupation_classes).to(device)\n\n    # Schedule\n    training_schedule = TrainSchedule(\n        model=model,\n        epochs_last_only=epochs_last_only,\n        epochs_joint=epochs_joint,\n        train_dataloader=train_dataloader,\n        val_dataloader=val_dataloader,\n        joint_lr_multiplier=joint_lr_multiplier,\n        birth_year_loss_fn=birth_year_loss_fn,\n        gender_loss_fn=gender_loss_fn,\n        occupation_loss_fn=occupation_loss_fn,\n        lr=lr,\n        device=device,\n        early_stopping_epochs=early_stopping_epochs,\n        reporting_and_early_stopping_weights=target_metric_weights,\n        backprop_weights=backprop_weights,\n        occupation_class_weights=occupation_class_weights,\n    )\n\n    # Base WandB config\n    base_config = {\n        key: val\n        for key, val in locals().items()\n        if isinstance(val, (int, float, str, bool, dict))\n    }\n\n    # This code allows for sweep runs (called from sweep_runner.py) and single runs\n    if wandb.run is None:\n        wandb_run = wandb.init(project=\"project-name\", config=base_config)\n    else:\n        try:\n            wandb.config.update(base_config, allow_val_change=True)\n        except Exception:\n            pass\n        wandb_run = wandb.run\n\n    try:\n        training_schedule.train()\n    finally:\n        wandb.finish()\n\n\n\n\nModel Training\nTraining Architecture\n\nGPU: Model training was performed on cloud GPUs hosted on Runpod.\nMetric Reporting: Metric reporting, hyperparameter sweeps, and training monitoring were all done using Weights & Biases(WandB).\nHyperparameter sweeps: I performed initial testing by varying loss functions, optimizers, scheduling, and model architecture. After finding a more limited range of options, I performed a Bayesian sweep over a variety of hyperparameters. I varied batch size, dropout, add-on layer depth and width, the scale factor on occupation loss, loss functions, and the second-phase learning-rate multiplier.\n\n\n\n\nWeights and Biases sweep dashboard graph\n\n\n\n\nModel Serving\nI used Gradio to build a shareable model interface. The classification portion required inverting data transformations and presenting the data in a (hopefully) visually appealing way. I hosted the Gradio interface in a Hugging Face Space.\nI also wanted the model to be useful as a tool for finding relevant quotes. To do this, I first embedded all scraped quotes. Then, I used FAISS to create an efficiently queryable index of the dense embeddings. For any entered quote, the interface retrieves the 100 most similar quotes by dot-product similarity. It then displays the five most similar quotes, filtered by the user’s selected “minimum author popularity,” as measured by the \\(\\log_2\\) of the author’s Wikipedia views."
  },
  {
    "objectID": "projects/quote-machine/index.html#example-quotes",
    "href": "projects/quote-machine/index.html#example-quotes",
    "title": "Quote Machine",
    "section": "Example Quotes",
    "text": "Example Quotes\n\n\n\nQuote\nAuthor\nBirth Year\nOccupation\n\n\n\n\nVirtue is debased by self-justification\nVoltaire\n1694\nFilm & Theater Writer, Literature & Writing, Humanities, Law & Jurisprudence\n\n\nPeople like you to be something, preferably what they are.\nJohn Steinbeck\n1902\nLiterature & Writing, Film & Theater Writer\n\n\nThe severest justice may not always be the best policy\nAbraham Lincoln\n1809\nLaw & Jurisprudence, Literature & Writing, Politics & Government, Military\n\n\nAllah is the Greatest. I’m just the greatest boxer.\nMuhammad Ali\n1942\nLiterature & Writing, Athletes & Sports, Social Advocacy\n\n\nTact is the ability to describe others as they see themselves\nUnknown, commonly misattributed\n~1925\nSee Quote Investigator"
  },
  {
    "objectID": "projects/quote-machine/index.html#footnotes",
    "href": "projects/quote-machine/index.html#footnotes",
    "title": "Quote Machine",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI have a couple theories to explain this observation.\n\nThere is a specific style that both appeals to contemporary readers and sounds somewhat historical.\nMid-century newspaper editors, who apparently had a rather loose relationship with the truth, are the source of many misattributed quotes. The similar sound of missattributed quotes may come from a similar sound of these editors.\n\n↩︎\nI would take a different approach if I revisit this project. The current method greatly incentivizes away from predicting more recent years.↩︎"
  },
  {
    "objectID": "projects/kidney_certainty/index.html",
    "href": "projects/kidney_certainty/index.html",
    "title": "Certainty in Kidney Exchange",
    "section": "",
    "text": "This paper was prepared Fall 2024 for ECON 605: Advanced Microeconomics with Dr. Charles Becker. See the latest draft below.\n\n\nThis text should not be visable. This is a hacky way to correct the expected reading time. I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nGreat Salt Lake Policy Dashboard\n\n24 min\n\nThe Great Salt Lake is drying up. Why should we care, and what can we do about it?\n\n\n\nJan 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCertainty in Kidney Exchange\n\n27 min\n\nKidney transplants, scarcity, and finding the optimal certainty of success\n\n\n\nJan 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuote Machine\n\n11 min\n\nDid Einstein really say that?\n\n\n\nJan 19, 2026\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "Copyright 2025 Liam Connor\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  }
]