{"title":"Quote Machine","markdown":{"yaml":{"title":"Quote Machine","date":"2026-01-19","categories":["natural language processing","data scraping","ML","AI"],"description":"Did Einstein really say that?","image":"image.png","listing-order":3,"toc":true,"toc-location":"right","toc-depth":3,"toc-expand":true,"code-fold":true},"headingText":"Project TLDR","containsRefs":false,"markdown":"\n\n<br>\n<!-- <iframe src=\"https://slc99-quote-machine.hf.space\" width=\"100%\" height=\"800\" style=\"box-shadow: 0 2px 8px var(--bs-body-color);\"></iframe> -->\n<iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"800\" style=\"box-shadow: 0 2px 8px var(--bs-body-color);\"></iframe>\n\n**End-to-end NLP project** that predicts author metadata from text.\n\n- **Data Engineering**: Built a custom scraper for Wikiquote & Wikimedia to curate a high-quality dataset of 200k+ quotes mapped to author metadata (birth year, occupation, gender).\n- **Modeling**: Fine-tuned a Transformer-based architecture (Sentence-BERT backbone & custom PyTorch heads) for multi-task learning.\n- **Deployment**: Deployed a scalable inference API and interactive UI via Hugging Face Spaces & Gradio, featuring vector-based semantic search using FAISS.\n\n\n## Introduction\n<!-- \nI believe that history can serve as a useful tool for understanding ourselves and our world. I think a useful starting point for the study of history is the understanding that historical people were not too dissimilar to ourselves and the people we know. I believe that it is far too easy to 'other' those who lived before us, which prevents us from learning from the past or drawing from it.\n\nRecognizing the humanity we share with historic figures is a necessary step in understanding history and learning from it. It is far too easy to 'other' past generations and assume that our current reality is different to the point of irrelevance. A good quote can knock away this misconception, by reminding us of our shared humanity. On the opposite side, a \n\nIs it possible to tell a whether a quote is misattributed from the quote alone? Yes, Google it. \n\nI believe that missattributed quotes \n\nHistorical figures are often mythicized\n\nHistory is often \n\nIt is hard to present history\n\nIt is hard to learn from history if we do not\n\nIt is easy to see historical people as fundamentally different from ourselves. It is hard to learn from history if we believe their lives were so different  -->\n\nI love the website [Quote Investigator](https://quoteinvestigator.com/). Author Garson O’Toole has performed careful research to track down the true origin of over 2,000 user submitted quotes. While browsing the site, I noticed that many misattributed quotes had a similar style, while actual historical quotes were much more varied in word choice and phrasing.[^longnote1] I wanted to train a model along the lines of this intuition. Ideally, the model could serve as a first pass on a quote's legitimacy. To accomplish this, I built a natural language processing model that predicts a quote author's gender, birth year, and occupation. First, I web-scraped 230k quotes from Wikiquote and data on 12k authors from Wikimedia. Next, I built a PyTorch model using a BERT encoding backbone and multiple prediction heads. Then, I trained the model on remote GPUs and monitored performance with Weights and Biases. Finally, I deployed the model with an interactive UI via Gradio and Hugging Face Spaces. \n\nThe first step in learning from history is to recognize our shared humanity with the past. A good quote can help bridge the gap to past generations and lead us to see the applicability of their experience. A misattributed quote can often do the opposite. It may further the mythicization of historical figures and, consequently, their irrelevance.\n\n[^longnote1]: I have a couple theories to explain this observation. \n\n    1. There is a specific style that both appeals to contemporary readers and sounds somewhat historical.\n    2. Mid-century newspaper editors, who apparently had a rather loose relationship with the truth, are the source of many misattributed quotes. The similar sound of missattributed quotes may come from a similar sound of these editors.\n\n<!-- A misattributed quote, however, often does the opposite. \n\n\nThe best way to tell whether a quote is misattributed is to Google it.\n\nI wanted to see whether a deep learning model could make reasonable predictions about an author's birth year and occupation based on a singl \n\nI wanted to build a model that predicts certain attributes of the author \n\n\n\nIn doing so, he humanizes the past. Learning from history requires acknowledging that humans have not changed much in the past few thousand years, and the challenges that historical figures faced are not too dissimilar to the ones we face today.  \n\n\nMany misattributed quotes work to mythologize historical figures by playing into established chara\n\n Overcoming this misconception is necessary to learn from history. -->\n\n## Data\n### Data Scraping\nQuotes were scraped from [Wikiquote](https://en.wikiquote.org/wiki/Main_Page). Wikiquote was selected over the popular [Goodreads](https://www.kaggle.com/datasets/akmittal/quotes-dataset) dataset for two reasons: first, Wikiquote provides a Wikimedia ID, allowing accurate author-attribute matching; second, Goodreads has many misattributed quotes, which runs counter to the original motivation.\n\nI used [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) to scrape Wikiquote. I first collected individual profile links by iterating through an index of people by name. Then, I scraped text from the *edit* page instead of the primary page because the edit page was more easily parsable. The code for the scraping is below.\n\nI accessed [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) through their API. I retrieved data on each author's birth year, death year, occupation, nationality, gender, and average monthly page views.\n\n### Data Cleaning\n\nOwing to the crowdsourced nature of Wikiquote, the initial data pull was *very* messy. I performed the following processing steps:\n\n1. Parse individual quotes and their associated headers from the full edit text. Recording the header was necessary because many pages include a \"Misattributed\" or \"Quotes About [Person]\" section that should be filtered.\n2. Mark bolded quote segments. Many Wikiquote entries include a memorable bolded quote alongside significant context. I recorded and marked the bolded fragments for later processing.\n3. Clean common markup artifacts.\n4. Filter non-English quotes.\n5. Perform systematic checks and spot checks to remove other abnormalities from the scraped data, such as external links, specific poorly formatted authors, and quotes with messy headers.\n\nCleaning the author data was much simpler.\n\n### Data Processing for Training\n\nNow that I had clean(er) data, it was time to prepare the data for model training. I performed the following steps:\n\n1. *Create a multi-label target for occupation*: Due to Wikidata being crowdsourced, the author data contained hundreds of obscure occupations. To create reasonable targets, I filtered to the 200 most common occupations (dropping 290 of 11,761 authors) and then had <`an LLM`> categorize occupations. I then manually tweaked categories. I selected this approach over an occupation co-occurrence clustering approach, as the manual method produced more semantically meaningful categories.\n2. *Create category weights to address occupation skew*: The most popular category, `Literature & Writing`, has 100x as many samples as the least popular category, `Athletes & Sports`. To address this, I added a scaling factor for occupation categories, defined as $\\text{w}_i = \\frac{\\max w}{\\text{count}^\\alpha}$, where $\\alpha \\in [0, 1]$ is a scaling hyperparameter. These weights were used to scale loss during backpropagation. A full breakdown of categories, associated quote counts, and weights is provided below.\n\n3. *Create birth year targets*: Ideally, the model would predict when a quote was said. I use the author's birth year as a proxy, given the available data. I wanted the birth year target to meet the following constraints:\n    1. Penalize the same absolute error more when the year is closer to the present day, so that predicting 200 CE for 400 CE is minor, but predicting 1800 CE for 2000 CE is major.\n    2. Predict a value with a continuous domain, to enable encoding the continuous development of language. This ruled out binarized approaches.\n    \nTo achieve these goals, I defined $\\text{Year label}= \\log (b-x) - \\log(b-c)$, where $c = 2025$ (the current year) is used to set the intercept, and $b$ is a hyperparameter that determines the steepness of the slope, which captures the relative weight of date differences in the present day vs. the far past. I $z$-normalized the converted target.[^longnote2]\n\n[^longnote2]: I would take a different approach if I revisit this project. The current method greatly incentivizes away from predicting more recent years.\n\n\n4. *Weight quotes by author's popularity and number of quotes*: There were two problems with the current data. First, the number of quotes per author varies by orders of magnitude. Second, I wanted \"higher quality\" authors (as measured by Wikipedia page views) to be upsampled. To achieve this, we set $w_{\\text{quote}} = \\frac{\\log_2(\\text{author page views})}{\\text{total author quotes}}$. Authors with missing page view counts were assumed to be missing uniformly at random.\n\n## Model \n### Model Design \nThe model uses a pretrained sentence-embedding model as a backbone. I selected the [Sentence Transformers](https://sbert.net/) model [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) because it is lightweight and popular. Encoded quotes are then passed through a set of shared add-on layers, composed of linear, ReLU, and dropout layers. The depth and width of these layers were parameterized. The last shared layer feeds into three prediction heads for gender, year, and occupation.\n\n### Training Parameters\n\n<u>Loss functions </u> \n\n- *Gender loss*: The gender prediction head used binary cross-entropy (BCE). After empirical testing, the noisiness of the gender head was preventing the model from learning. To address this, I detached the embeddings before feeding them into the gender head, isolating gender backpropagation to the prediction head only. \n- *Occupation loss*: I tested two loss functions for the occupation head, BCE and focal loss. Loss was calculated per occupation category, scaled by the previously calculated weights, and then averaged. This prevented the model from ignoring less common occupations. \n- *Birth year loss*: Birth year loss was calculated using heteroskedastic negative log-likelihood (HNLL). HNLL requires the model to predict a normal distribution for each sample, allowing the model to strategically express uncertainty in its predictions. HNLL was chosen over MSE to reduce regression to the mean. HNLL also has the added benefit of including a measure of model uncertainty.\n\n<u>Training Regime</u>\n\n- *Phases*: The training regime was split into two phases. In the first phase, only the added layers and prediction heads are modified. In the second phase, the encoder model is also modified. \n- *Optimizer*: AdamW was selected as the optimizer. The learning rate for the encoder, add-on layers, and each prediction head are set separately.\n- *Scheduling*: The learning rate was scheduled with a linear warmup (10% of total batches) and linear decay (90% of batches). Scheduling reset between phases. The second phase had a proportionally lower learning rate than the first phase.  The first phase lasted four epochs (empirically set based on when performance plateaued). The second phase lasted up to 12 epochs, depending on early stopping.\n- *Evaluation*: Model performance was evaluated against a withheld validation set. The validation set contained 15% of the overall data. Model performance was calculated as the weighted average of task performance. \n    - *Gender*: AUROC\n    - *Year*: MAE\n    - *Occupation*: Hamming distance, with multi-label \"macro\" reduction. This places more emphasis on uncommon labels\n\n### Model Training\n<u>Training Architecture</u>\n\n- *GPU*: Model training was performed on cloud GPUs hosted on [Runpod](https://www.runpod.io/). \n- *Metric Reporting*: Metric reporting, hyperparameter sweeps, and training monitoring were all done using [Weights & Biases](https://wandb.ai/)(WandB). \n- *Hyperparameter sweeps*: I performed initial testing by varying loss functions, optimizers, scheduling, and model architecture. After finding a more limited range of options, I performed a Bayesian sweep over a variety of hyperparameters. I varied batch size, dropout, add-on layer depth and width, the scale factor on occupation loss, loss functions, and the second-phase learning-rate multiplier. \n\n![Weights and Biases sweep dashboard graph](attachment:wandb_sweep_screenshot.png)\n\n### Model Serving\n\nI used [Gradio](https://www.gradio.app/) to build a shareable model interface. The classification portion required inverting data transformations and presenting the data in a (hopefully) visually appealing way. I hosted the Gradio interface in a [Hugging Face Space](https://huggingface.co/spaces).\n\nI also wanted the model to be useful as a tool for finding relevant quotes. To do this, I first embedded all scraped quotes. Then, I used [FAISS](https://faiss.ai/) to create an efficiently queryable index of the dense embeddings. For any entered quote, the interface retrieves the 100 most similar quotes by dot-product similarity. It then displays the five most similar quotes, filtered by the user's selected \"minimum author popularity,\" as measured by the $\\log_2$ of the author's Wikipedia views.\n\n## Example Quotes\n| Quote | Author | Birth Year | Occupation |\n|-------|--------|------------|------------|\n| Virtue is debased by self-justification | Voltaire | 1694 | Film & Theater Writer, Literature & Writing, Humanities, Law & Jurisprudence |\n| People like you to be something, preferably what they are. | John Steinbeck | 1902 | Literature & Writing, Film & Theater Writer |\n| The severest justice may not always be the best policy | Abraham Lincoln | 1809 | Law & Jurisprudence, Literature & Writing, Politics & Government, Military |\n| Allah is the Greatest. I'm just the greatest boxer. | Muhammad Ali | 1942 | Literature & Writing, Athletes & Sports, Social Advocacy |\n| Tact is the ability to describe others as they see themselves | Unknown, commonly misattributed | ~1925 | See [Quote Investigator](https://quoteinvestigator.com/2021/03/06/tact-see/) | \n\n<!-- |In so far as theories of mathematics speak about reality, they are not certain, and in so far as they are certain, they do not speak about reality. | -->\n","srcMarkdownNoYaml":"\n\n<br>\n<!-- <iframe src=\"https://slc99-quote-machine.hf.space\" width=\"100%\" height=\"800\" style=\"box-shadow: 0 2px 8px var(--bs-body-color);\"></iframe> -->\n<iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"800\" style=\"box-shadow: 0 2px 8px var(--bs-body-color);\"></iframe>\n\n### Project TLDR\n**End-to-end NLP project** that predicts author metadata from text.\n\n- **Data Engineering**: Built a custom scraper for Wikiquote & Wikimedia to curate a high-quality dataset of 200k+ quotes mapped to author metadata (birth year, occupation, gender).\n- **Modeling**: Fine-tuned a Transformer-based architecture (Sentence-BERT backbone & custom PyTorch heads) for multi-task learning.\n- **Deployment**: Deployed a scalable inference API and interactive UI via Hugging Face Spaces & Gradio, featuring vector-based semantic search using FAISS.\n\n\n## Introduction\n<!-- \nI believe that history can serve as a useful tool for understanding ourselves and our world. I think a useful starting point for the study of history is the understanding that historical people were not too dissimilar to ourselves and the people we know. I believe that it is far too easy to 'other' those who lived before us, which prevents us from learning from the past or drawing from it.\n\nRecognizing the humanity we share with historic figures is a necessary step in understanding history and learning from it. It is far too easy to 'other' past generations and assume that our current reality is different to the point of irrelevance. A good quote can knock away this misconception, by reminding us of our shared humanity. On the opposite side, a \n\nIs it possible to tell a whether a quote is misattributed from the quote alone? Yes, Google it. \n\nI believe that missattributed quotes \n\nHistorical figures are often mythicized\n\nHistory is often \n\nIt is hard to present history\n\nIt is hard to learn from history if we do not\n\nIt is easy to see historical people as fundamentally different from ourselves. It is hard to learn from history if we believe their lives were so different  -->\n\nI love the website [Quote Investigator](https://quoteinvestigator.com/). Author Garson O’Toole has performed careful research to track down the true origin of over 2,000 user submitted quotes. While browsing the site, I noticed that many misattributed quotes had a similar style, while actual historical quotes were much more varied in word choice and phrasing.[^longnote1] I wanted to train a model along the lines of this intuition. Ideally, the model could serve as a first pass on a quote's legitimacy. To accomplish this, I built a natural language processing model that predicts a quote author's gender, birth year, and occupation. First, I web-scraped 230k quotes from Wikiquote and data on 12k authors from Wikimedia. Next, I built a PyTorch model using a BERT encoding backbone and multiple prediction heads. Then, I trained the model on remote GPUs and monitored performance with Weights and Biases. Finally, I deployed the model with an interactive UI via Gradio and Hugging Face Spaces. \n\nThe first step in learning from history is to recognize our shared humanity with the past. A good quote can help bridge the gap to past generations and lead us to see the applicability of their experience. A misattributed quote can often do the opposite. It may further the mythicization of historical figures and, consequently, their irrelevance.\n\n[^longnote1]: I have a couple theories to explain this observation. \n\n    1. There is a specific style that both appeals to contemporary readers and sounds somewhat historical.\n    2. Mid-century newspaper editors, who apparently had a rather loose relationship with the truth, are the source of many misattributed quotes. The similar sound of missattributed quotes may come from a similar sound of these editors.\n\n<!-- A misattributed quote, however, often does the opposite. \n\n\nThe best way to tell whether a quote is misattributed is to Google it.\n\nI wanted to see whether a deep learning model could make reasonable predictions about an author's birth year and occupation based on a singl \n\nI wanted to build a model that predicts certain attributes of the author \n\n\n\nIn doing so, he humanizes the past. Learning from history requires acknowledging that humans have not changed much in the past few thousand years, and the challenges that historical figures faced are not too dissimilar to the ones we face today.  \n\n\nMany misattributed quotes work to mythologize historical figures by playing into established chara\n\n Overcoming this misconception is necessary to learn from history. -->\n\n## Data\n### Data Scraping\nQuotes were scraped from [Wikiquote](https://en.wikiquote.org/wiki/Main_Page). Wikiquote was selected over the popular [Goodreads](https://www.kaggle.com/datasets/akmittal/quotes-dataset) dataset for two reasons: first, Wikiquote provides a Wikimedia ID, allowing accurate author-attribute matching; second, Goodreads has many misattributed quotes, which runs counter to the original motivation.\n\nI used [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) to scrape Wikiquote. I first collected individual profile links by iterating through an index of people by name. Then, I scraped text from the *edit* page instead of the primary page because the edit page was more easily parsable. The code for the scraping is below.\n\nI accessed [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) through their API. I retrieved data on each author's birth year, death year, occupation, nationality, gender, and average monthly page views.\n\n### Data Cleaning\n\nOwing to the crowdsourced nature of Wikiquote, the initial data pull was *very* messy. I performed the following processing steps:\n\n1. Parse individual quotes and their associated headers from the full edit text. Recording the header was necessary because many pages include a \"Misattributed\" or \"Quotes About [Person]\" section that should be filtered.\n2. Mark bolded quote segments. Many Wikiquote entries include a memorable bolded quote alongside significant context. I recorded and marked the bolded fragments for later processing.\n3. Clean common markup artifacts.\n4. Filter non-English quotes.\n5. Perform systematic checks and spot checks to remove other abnormalities from the scraped data, such as external links, specific poorly formatted authors, and quotes with messy headers.\n\nCleaning the author data was much simpler.\n\n### Data Processing for Training\n\nNow that I had clean(er) data, it was time to prepare the data for model training. I performed the following steps:\n\n1. *Create a multi-label target for occupation*: Due to Wikidata being crowdsourced, the author data contained hundreds of obscure occupations. To create reasonable targets, I filtered to the 200 most common occupations (dropping 290 of 11,761 authors) and then had <`an LLM`> categorize occupations. I then manually tweaked categories. I selected this approach over an occupation co-occurrence clustering approach, as the manual method produced more semantically meaningful categories.\n2. *Create category weights to address occupation skew*: The most popular category, `Literature & Writing`, has 100x as many samples as the least popular category, `Athletes & Sports`. To address this, I added a scaling factor for occupation categories, defined as $\\text{w}_i = \\frac{\\max w}{\\text{count}^\\alpha}$, where $\\alpha \\in [0, 1]$ is a scaling hyperparameter. These weights were used to scale loss during backpropagation. A full breakdown of categories, associated quote counts, and weights is provided below.\n\n3. *Create birth year targets*: Ideally, the model would predict when a quote was said. I use the author's birth year as a proxy, given the available data. I wanted the birth year target to meet the following constraints:\n    1. Penalize the same absolute error more when the year is closer to the present day, so that predicting 200 CE for 400 CE is minor, but predicting 1800 CE for 2000 CE is major.\n    2. Predict a value with a continuous domain, to enable encoding the continuous development of language. This ruled out binarized approaches.\n    \nTo achieve these goals, I defined $\\text{Year label}= \\log (b-x) - \\log(b-c)$, where $c = 2025$ (the current year) is used to set the intercept, and $b$ is a hyperparameter that determines the steepness of the slope, which captures the relative weight of date differences in the present day vs. the far past. I $z$-normalized the converted target.[^longnote2]\n\n[^longnote2]: I would take a different approach if I revisit this project. The current method greatly incentivizes away from predicting more recent years.\n\n\n4. *Weight quotes by author's popularity and number of quotes*: There were two problems with the current data. First, the number of quotes per author varies by orders of magnitude. Second, I wanted \"higher quality\" authors (as measured by Wikipedia page views) to be upsampled. To achieve this, we set $w_{\\text{quote}} = \\frac{\\log_2(\\text{author page views})}{\\text{total author quotes}}$. Authors with missing page view counts were assumed to be missing uniformly at random.\n\n## Model \n### Model Design \nThe model uses a pretrained sentence-embedding model as a backbone. I selected the [Sentence Transformers](https://sbert.net/) model [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) because it is lightweight and popular. Encoded quotes are then passed through a set of shared add-on layers, composed of linear, ReLU, and dropout layers. The depth and width of these layers were parameterized. The last shared layer feeds into three prediction heads for gender, year, and occupation.\n\n### Training Parameters\n\n<u>Loss functions </u> \n\n- *Gender loss*: The gender prediction head used binary cross-entropy (BCE). After empirical testing, the noisiness of the gender head was preventing the model from learning. To address this, I detached the embeddings before feeding them into the gender head, isolating gender backpropagation to the prediction head only. \n- *Occupation loss*: I tested two loss functions for the occupation head, BCE and focal loss. Loss was calculated per occupation category, scaled by the previously calculated weights, and then averaged. This prevented the model from ignoring less common occupations. \n- *Birth year loss*: Birth year loss was calculated using heteroskedastic negative log-likelihood (HNLL). HNLL requires the model to predict a normal distribution for each sample, allowing the model to strategically express uncertainty in its predictions. HNLL was chosen over MSE to reduce regression to the mean. HNLL also has the added benefit of including a measure of model uncertainty.\n\n<u>Training Regime</u>\n\n- *Phases*: The training regime was split into two phases. In the first phase, only the added layers and prediction heads are modified. In the second phase, the encoder model is also modified. \n- *Optimizer*: AdamW was selected as the optimizer. The learning rate for the encoder, add-on layers, and each prediction head are set separately.\n- *Scheduling*: The learning rate was scheduled with a linear warmup (10% of total batches) and linear decay (90% of batches). Scheduling reset between phases. The second phase had a proportionally lower learning rate than the first phase.  The first phase lasted four epochs (empirically set based on when performance plateaued). The second phase lasted up to 12 epochs, depending on early stopping.\n- *Evaluation*: Model performance was evaluated against a withheld validation set. The validation set contained 15% of the overall data. Model performance was calculated as the weighted average of task performance. \n    - *Gender*: AUROC\n    - *Year*: MAE\n    - *Occupation*: Hamming distance, with multi-label \"macro\" reduction. This places more emphasis on uncommon labels\n\n### Model Training\n<u>Training Architecture</u>\n\n- *GPU*: Model training was performed on cloud GPUs hosted on [Runpod](https://www.runpod.io/). \n- *Metric Reporting*: Metric reporting, hyperparameter sweeps, and training monitoring were all done using [Weights & Biases](https://wandb.ai/)(WandB). \n- *Hyperparameter sweeps*: I performed initial testing by varying loss functions, optimizers, scheduling, and model architecture. After finding a more limited range of options, I performed a Bayesian sweep over a variety of hyperparameters. I varied batch size, dropout, add-on layer depth and width, the scale factor on occupation loss, loss functions, and the second-phase learning-rate multiplier. \n\n![Weights and Biases sweep dashboard graph](attachment:wandb_sweep_screenshot.png)\n\n### Model Serving\n\nI used [Gradio](https://www.gradio.app/) to build a shareable model interface. The classification portion required inverting data transformations and presenting the data in a (hopefully) visually appealing way. I hosted the Gradio interface in a [Hugging Face Space](https://huggingface.co/spaces).\n\nI also wanted the model to be useful as a tool for finding relevant quotes. To do this, I first embedded all scraped quotes. Then, I used [FAISS](https://faiss.ai/) to create an efficiently queryable index of the dense embeddings. For any entered quote, the interface retrieves the 100 most similar quotes by dot-product similarity. It then displays the five most similar quotes, filtered by the user's selected \"minimum author popularity,\" as measured by the $\\log_2$ of the author's Wikipedia views.\n\n## Example Quotes\n| Quote | Author | Birth Year | Occupation |\n|-------|--------|------------|------------|\n| Virtue is debased by self-justification | Voltaire | 1694 | Film & Theater Writer, Literature & Writing, Humanities, Law & Jurisprudence |\n| People like you to be something, preferably what they are. | John Steinbeck | 1902 | Literature & Writing, Film & Theater Writer |\n| The severest justice may not always be the best policy | Abraham Lincoln | 1809 | Law & Jurisprudence, Literature & Writing, Politics & Government, Military |\n| Allah is the Greatest. I'm just the greatest boxer. | Muhammad Ali | 1942 | Literature & Writing, Athletes & Sports, Social Advocacy |\n| Tact is the ability to describe others as they see themselves | Unknown, commonly misattributed | ~1925 | See [Quote Investigator](https://quoteinvestigator.com/2021/03/06/tact-see/) | \n\n<!-- |In so far as theories of mathematics speak about reality, they are not certain, and in so far as they are certain, they do not speak about reality. | -->\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../scss/styles.css"],"highlight-style":"gruvbox","toc":true,"toc-depth":3,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","theme":{"light":["flatly","../../scss/light.scss"]},"smooth-scroll":true,"title":"Quote Machine","date":"2026-01-19","categories":["natural language processing","data scraping","ML","AI"],"description":"Did Einstein really say that?","image":"image.png","listing-order":3,"toc-location":"right","toc-expand":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}